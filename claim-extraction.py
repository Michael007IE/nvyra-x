# ------------------------------------------------------------------------------
#  CONFIDENTIAL AND PROPRIETARY
#  Copyright (c) 2025-2026 nvyra-x. All Rights Reserved.
#
#  NOTICE:  All information contained herein is, and remains the property of
#  nvyra-x. The intellectual and technical concepts contained herein are
#  proprietary to nvyra-x and may be covered by Irish and Foreign Patents,
#  patents in process, and are protected by trade secret or copyright law.
#  Dissemination of this information or reproduction of this material is
#  strictly forbidden unless prior written permission is obtained from nvyra-x.
# ------------------------------------------------------------------------------

# Cell 0
!pip install -q uv

# Clean up problematic broken links before any dependency step
!rm -f /usr/local/lib/python3.12/dist-packages/~vidia-cusolver-cu12
print("'uv' installed and environment cleaned.")

# Cell 1 : vLLM instalation.
VLLM_VERSION = "0.7.3"
CUDA_INDEX_URL = "https://download.pytorch.org/whl/cu121" # Use 121 as the target CUDA version

print(f"ðŸš€ Attempting installation of vLLM v{VLLM_VERSION} via PyPI/CUDA Index...")
!uv pip install vllm=={VLLM_VERSION} --extra-index-url {CUDA_INDEX_URL}
!uv pip install --no-build-isolation flash-attn xformers

print(f" vLLM (v{VLLM_VERSION} for CUDA 12.1) and core packages installed using 'uv'.")


# Cell 2 : Imports
from vllm import LLM, SamplingParams
from typing import List, Dict, Tuple, Optional
import json
import re
import time
import gc
import torch
import pandas as pd
import hashlib
import csv
from pathlib import Path
from tqdm.auto import tqdm
import sqlite3
import sys 
print(" All libraries imported successfully.")

# Cell 3: Database Manager
import sqlite3
import sys
from typing import Dict, Optional

class DatabaseManager:
    def __init__(self, db_name: str = "pipeline.db"):
        self.db_name = db_name
        self.conn: Optional[sqlite3.Connection] = None
        try:
            self.conn = sqlite3.connect(self.db_name, timeout=10.0)
            self._init_db()
            print(f" Initialized robust DatabaseManager (using {self.db_name})")
        except sqlite3.Error as e:
            print(f" Database Error:  Failed to connect or init DB: {e}", file=sys.stderr)
            if self.conn:
                self.conn.close()
            raise 

    def _init_db(self):
        if not self.conn:
            return

        with self.conn:
            self.conn.execute("PRAGMA journal_mode=WAL;")
            self.conn.execute("PRAGMA synchronous=NORMAL;")
            self.conn.execute("PRAGMA foreign_keys=ON;")
            self.conn.execute("""
                CREATE TABLE IF NOT EXISTS processed_log (
                    article_id TEXT PRIMARY KEY,
                    stage TEXT NOT NULL,
                    status TEXT NOT NULL
                )
            """)
            self.conn.execute("""
                CREATE TABLE IF NOT EXISTS stage1_claims (
                    claim_id TEXT PRIMARY KEY,
                    article_id TEXT NOT NULL,
                    claim_num INTEGER NOT NULL,
                    claim_text TEXT,
                    hyde_doc TEXT
                )
            """)
            self.conn.execute("""
                CREATE TABLE IF NOT EXISTS stage1_triplets (
                    triplet_id INTEGER PRIMARY KEY AUTOINCREMENT,
                    claim_id TEXT NOT NULL,
                    subject TEXT,
                    predicate TEXT,
                    object TEXT,
                    FOREIGN KEY(claim_id)
                        REFERENCES stage1_claims(claim_id)
                        ON DELETE CASCADE
                )
            """)
            self.conn.execute("CREATE INDEX IF NOT EXISTS idx_triplets_claim_id ON stage1_triplets (claim_id);")
            self.conn.execute("CREATE INDEX IF NOT EXISTS idx_claims_article_id ON stage1_claims (article_id);")

    def is_processed(self, article_id: str, stage: str) -> bool:
        """Checks if an article is already marked as processed."""
        if not self.conn:
            print(" DB Connection is closed.", file=sys.stderr)
            return False

        try:
            cur = self.conn.execute(
                "SELECT 1 FROM processed_log WHERE article_id = ? AND stage = ?",
                (article_id, stage)
            )
            return cur.fetchone() is not None
        except sqlite3.Error as e:
            print(f" DB ERROR in is_processed: {e}", file=sys.stderr)
            return False

    def mark_processed(self, article_id: str, stage: str, status: str):
        if not self.conn:
            print(" DB Connection is closed.", file=sys.stderr)
            return

        try:
            with self.conn:
                self.conn.execute(
                    "INSERT OR REPLACE INTO processed_log (article_id, stage, status) VALUES (?, ?, ?)",
                    (article_id, stage, status)
                )
        except sqlite3.Error as e:
            print(f" DB error in mark_processed: {e}", file=sys.stderr)

    def log_claims_and_triplets(self, article_id: str, structured_data: Dict):

        if not self.conn:
            print(" DB Connection is closed.", file=sys.stderr)
            return

        try:
            with self.conn:
                for i, point_data in enumerate(structured_data.get('main_points', []), 1):
                    claim_id = f"{article_id}_c{i}"

                    # Insert/replace the claim
                    self.conn.execute("""
                    INSERT OR REPLACE INTO stage1_claims (claim_id, article_id, claim_num, claim_text, hyde_doc)
                    VALUES (?, ?, ?, ?, ?)
                    """, (
                        claim_id,
                        article_id,
                        i,
                        point_data.get('point'),
                        point_data.get('hyde_doc')
                    ))
                    triplets_to_insert = []
                    for triplet in point_data.get('triplets', []):
                        if isinstance(triplet, list) and len(triplet) == 3:
                            triplets_to_insert.append(
                                (claim_id, triplet[0], triplet[1], triplet[2])
                            )

                    if triplets_to_insert:
                        self.conn.executemany("""
                            INSERT INTO stage1_triplets (claim_id, subject, predicate, object)
                            VALUES (?, ?, ?, ?)
                        """, triplets_to_insert)

        except sqlite3.Error as e:
            print(f" DB error in log_claims_and_triplets: {e}", file=sys.stderr)

    def close(self):
        """Commits changes and safely closes the database connection."""
        if self.conn:
            try:
                self.conn.commit() # Final commit
                self.conn.close()
                self.conn = None
                print(f" Database connection to {self.db_name} closed.")
            except sqlite3.Error as e:
                print(f" DB error on close: {e}", file=sys.stderr)
    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()
        
# Cell 4: Stage 1 Pipeline

class UltraReliableStage1_AWQ:

    def __init__(self, hf_token: Optional[str] = None):
        self.hf_token = hf_token
        self.llm = None
        self._load_model()

    def _load_model(self):
        print("\n  Loading Qwen/Qwen2.5-7B-Instruct-AWQ (4-bit) with vLLM...")
        self.llm = LLM(
            model="Qwen/Qwen2.5-7B-Instruct-AWQ",
            trust_remote_code=True,
            dtype="half",
            quantization="awq",
            gpu_memory_utilization=0.95,
            max_model_len=16384,
            enforce_eager=False,
            max_num_seqs=32,
            tensor_parallel_size=1,
            download_dir="/content/models"
        )

        print(f" Qwen2.5-7B-AWQ loaded in 4-bit")
        print(f" Max context: 16384 tokens")
        print(f" Quantization: 4-bit (AWQ)")
        print(f" Batch size (max_num_seqs): 32")

    def _create_ultra_reliable_prompt(self, title: str, text: str) -> str:
        prompt = f"""You are a world-class data extraction system specialized in news article analysis. Your task is to extract structured information with PERFECT accuracy.

**YOUR TASK**:
Extract 3-5 factual claims from the article, then for each claim:
1. Write a HyDE document (2-3 sentences that elaborate the claim for search)
2. Extract knowledge graph triplets as [Subject, Predicate, Object]

**CRITICAL REQUIREMENTS**:
âœ“ Output ONLY valid JSON - no extra text before or after
âœ“ Extract FACTUAL claims, not opinions or commentary
âœ“ HyDE docs must be search-optimized (include WHO, WHAT, WHEN, WHERE)
âœ“ All triplets must have exactly 3 elements
âœ“ Handle the complete article - don't skip important details
âœ“ If an article is short, extract 3 claims minimum
âœ“ If an article is long, extract up to 5 claims maximum

**EXAMPLE OUTPUT** (follow this EXACT structure):
{{
  "main_points": [
    {{
      "point": "OpenAI released GPT-4 Turbo with 128K context window in November 2023",
      "hyde_doc": "In November 2023, OpenAI announced the release of GPT-4 Turbo, a significant update to their language model that features a 128,000 token context window. This expansion allows the model to process much longer documents and maintain coherence across extended conversations.",
      "triplets": [
        ["OpenAI", "released", "GPT-4 Turbo"],
        ["GPT-4 Turbo", "has_feature", "128K context window"],
        ["GPT-4 Turbo", "announced_in", "November 2023"]
      ]
    }},
    {{
      "point": "The model costs $0.01 per 1K input tokens",
      "hyde_doc": "GPT-4 Turbo's pricing structure was set at one cent per thousand input tokens, making it more cost-effective than previous versions. This pricing model aims to make advanced AI more accessible to developers and businesses.",
      "triplets": [
        ["GPT-4 Turbo", "costs", "$0.01 per 1K tokens"],
        ["Pricing", "applies_to", "input tokens"]
      ]
    }}
  ]
}}

**NOW EXTRACT FROM THIS ARTICLE**:

Title: {title}

Article Text:
{text}

Output the structured JSON now:
{{
  "main_points": ["""
        return prompt

    def _validate_and_repair_json(self, response: str, article_id: str) -> Optional[Dict]:
        try:
            start = response.find('{')
            if start == -1:
                start = response.find('[')
            end = response.rfind('}') + 1

            if start != -1 and end > start:
                json_str = response[start:end]
                json_str = json_str.replace('```json', '').replace('```', '')
                json_str = json_str.replace('\n', ' ').replace('\r', '')
                json_str = re.sub(r',(\s*[}\]])', r'\1', json_str)
                json_str = re.sub(r'}\s*{', '},{', json_str)
                json_str = re.sub(r':\s*"([^"]*?)$', r': "\1"', json_str)
                data = json.loads(json_str)

                if isinstance(data, dict) and 'main_points' in data:
                    return self._validate_structure(data, article_id)
                elif isinstance(data, list):
                    return self._validate_structure({'main_points': data}, article_id)
        except json.JSONDecodeError as e:
            print(f"  JSON parse error for {article_id[:10]}: {e}")
        except Exception as e:
            print(f"  Unexpected error for {article_id[:10]}: {e}")
        try:
            points = []
            point_pattern = r'"point"\s*:\s*"([^"]+)"'
            hyde_pattern = r'"hyde_doc"\s*:\s*"([^"]+)"'
            point_matches = re.findall(point_pattern, response)
            hyde_matches = re.findall(hyde_pattern, response)

            for i, point_text in enumerate(point_matches[:5]):
                hyde_text = hyde_matches[i] if i < len(hyde_matches) else f"Analysis of: {point_text}"
                points.append({
                    'point': point_text,
                    'hyde_doc': hyde_text,
                    'triplets': self._extract_triplets_from_text(response, i)
                })
            if points:
                return {'main_points': points}
        except Exception as e:
            print(f"  Regex extraction failed for {article_id[:10]}: {e}")
        try:
            points = []
            lines = response.split('\n')
            for line in lines:
                line = line.strip()
                if re.match(r'^[0-9\-\*â€¢]\s*[\.\):]?\s*', line):
                    point_text = re.sub(r'^[0-9\-\*â€¢]\s*[\.\):]?\s*', '', line)
                    if len(point_text) > 20:
                        points.append({
                            'point': point_text,
                            'hyde_doc': f"Detailed analysis: {point_text}",
                            'triplets': []
                        })
                    if len(points) >= 3:
                        break
            if points:
                return {'main_points': points}
        except Exception as e:
            print(f"  Line extraction failed for {article_id[:10]}: {e}")

        return None

    def _validate_structure(self, data: Dict, article_id: str) -> Dict:
        """
        Validates and repairs the data structure to ensure quality.
        """
        if 'main_points' not in data:
            return {'main_points': []}
        valid_points = []
        for point in data['main_points']:
            if not isinstance(point, dict) or 'point' not in point or not point['point'] or len(point['point']) < 10:
                continue
            if 'hyde_doc' not in point or not point['hyde_doc']:
                point['hyde_doc'] = f"This claim discusses: {point['point']}"
            elif len(point['hyde_doc']) < 30:
                point['hyde_doc'] = f"Analysis of the following claim: {point['point']}. {point['hyde_doc']}"
            if 'triplets' not in point:
                point['triplets'] = []
            valid_triplets = []
            for t in point['triplets']:
                if isinstance(t, list) and len(t) == 3 and all(isinstance(x, str) and len(x) > 0 for x in t):
                    valid_triplets.append([str(t[0]).strip(), str(t[1]).strip(), str(t[2]).strip()])
            point['triplets'] = valid_triplets
            valid_points.append(point)
        return {'main_points': valid_points[:5]}

    def _extract_triplets_from_text(self, text: str, claim_index: int) -> List[List[str]]:
        """
        Extracts triplets from raw text when JSON parsing fails.
        """
        triplets = []
        triplet_pattern = r'\[\s*"([^"]+)"\s*,\s*"([^"]+)"\s*,\s*"([^"]+)"\s*\]'
        matches = re.findall(triplet_pattern, text)
        for match in matches[:5]:
            if all(len(x.strip()) > 0 for x in match):
                triplets.append([match[0].strip(), match[1].strip(), match[2].strip()])
        return triplets

    def _score_output_quality(self, data: Dict) -> float:
        """
        Scores output quality to detect low-quality extractions.
        """
        if not data or 'main_points' not in data:
            return 0.0
        points = data['main_points']
        if not points:
            return 0.0
        score = 0.0
        if 3 <= len(points) <= 5:
            score += 0.3
        elif len(points) >= 2:
            score += 0.15
        avg_point_len = sum(len(p.get('point', '')) for p in points) / len(points)
        if 50 <= avg_point_len <= 200:
            score += 0.2
        elif 30 <= avg_point_len <= 300:
            score += 0.1
        hyde_ratio = sum(len(p.get('hyde_doc', '')) / max(len(p.get('point', 'x')), 1) for p in points) / len(points)
        if hyde_ratio > 1.5:
            score += 0.2
        elif hyde_ratio > 1.0:
            score += 0.1
        triplet_count = sum(len(p.get('triplets', [])) for p in points)
        if triplet_count >= len(points) * 2:
            score += 0.2
        elif triplet_count >= len(points):
            score += 0.1
        complete_points = sum(1 for p in points if all(k in p and p[k] for k in ['point', 'hyde_doc']))
        score += (complete_points / len(points)) * 0.1
        return min(score, 1.0)

    def process_batch(self, articles: List[Dict]) -> List[Dict]:
        """
        Process multiple articles with ultra-high reliability.
        """
        prompts = []
        article_ids = []

        for article in articles:
            prompt = self._create_ultra_reliable_prompt(article['title'], article['text'])
            prompts.append(prompt)
            article_ids.append(article['article_id'])

        sampling_params = SamplingParams(
            temperature=0.2,
            top_p=0.85,
            max_tokens=2560,
            stop=["<|endoftext|>", "<|im_end|>", "\n\n\n", "}}"],
            repetition_penalty=1.05,
            frequency_penalty=0.1,
            presence_penalty=0.1
        )

        print(f"  ðŸš€ Running batch inference on {len(prompts)} articles...")
        start_time = time.time()
        outputs = self.llm.generate(prompts, sampling_params)
        elapsed = time.time() - start_time
        print(f"  Batch complete in {elapsed:.2f}s ({elapsed/len(prompts):.3f}s per article)")

        results = []
        successful = 0
        for article, output, article_id in zip(articles, outputs, article_ids):
            response_text = output.outputs[0].text
            parsed_data = self._validate_and_repair_json(response_text, article_id)
            if parsed_data:
                quality_score = self._score_output_quality(parsed_data)
                if quality_score >= 0.5:
                    successful += 1
                else:
                    print(f"  Low quality output (score: {quality_score:.2f}) for {article_id[:10]}")
            else:
                print(f"   Failed to extract data for {article_id[:10]}")
                parsed_data = {'main_points': []}

            results.append({
                'article_id': article_id,
                'title': article['title'],
                'text': article['text'],
                'label': article['label'],
                'structured_data': parsed_data,
                'quality_score': self._score_output_quality(parsed_data)
            })

        success_rate = (successful / len(results)) * 100 if results else 0
        print(f"  Batch success rate: {success_rate:.1f}%")
        return results

    def unload(self):
        """Clean up VRAM."""
        print("\n Unloading Qwen2.5-7B-AWQ from VRAM...")
        del self.llm
        gc.collect()
        torch.cuda.empty_cache()
        print(" Model unloaded")


def run_stage_1_ultra_reliable(input_csv: str, batch_size: int = 32):
    """
    Ultra-reliable Stage 1 pipeline targeting 99%+ success rate.

    *** MODIFIED: Saves progress to Parquet after EVERY batch ***
    """
    print(f"\n{'='*70}")
    print(f"ULTRA-RELIABLE STAGE 1: 99%+ Success | 40x Throughput")
    print(f"Qwen2.5-7B-AWQ (4-bit) + Advanced Validation + Parquet Output")
    print(f"{'='*70}")

    processor = UltraReliableStage1_AWQ(hf_token=None)

    try:
        df = pd.read_csv(input_csv)
    except FileNotFoundError:
        print(f" Error: Input file '{input_csv}' not found.")
        processor.unload()
        return

    print(f"Loaded {len(df)} articles from {input_csv}")
    output_file = "step1_claims_data.parquet"

    header = [
        'article_id', 'title', 'text', 'label',
        'claim_1', 'claim_2', 'claim_3', 'claim_4', 'claim_5'
    ]
    processed_ids = set()
    existing_df = None
    with DatabaseManager(db_name="pipeline_stub.db") as db:
        if Path(output_file).exists():
            print(f"Resuming from existing file: {output_file}")
            try:
                existing_df = pd.read_parquet(output_file)
                processed_ids = set(existing_df['article_id'])
                print(f"  Found {len(processed_ids)} already processed articles.")
            except Exception as e:
                print(f"  Could not read Parquet file {output_file}. Starting fresh. Error: {e}")
                existing_df = None
                processed_ids = set()
        else:
            print(f"No existing Parquet file found. Starting fresh.")
            existing_df = pd.DataFrame(columns=header)
        articles_to_process = []
        for _, row in df.iterrows():
            text = str(row.get('text', ''))
            if not text or len(text) < 50:
                continue
            article_id = hashlib.md5(text.encode()).hexdigest()
            if article_id in processed_ids or db.is_processed(article_id, 'stage1'):
                continue
            articles_to_process.append({
                'article_id': article_id,
                'title': str(row.get('title', '')),
                'text': text,
                'label': str(row.get('label', ''))
            })

        if not articles_to_process:
            print("All articles already processed!")
            processor.unload()
            return

        print(f"\n Processing {len(articles_to_process)} new articles in batches of {batch_size}")

        successful = 0
        skipped = 0
        total_quality_score = 0.0
        for batch_start in tqdm(range(0, len(articles_to_process), batch_size),
                                desc="Processing batches"):

            batch = articles_to_process[batch_start:batch_start + batch_size]
            results = processor.process_batch(batch)
            current_batch_data = []

            for result in results:
                article_id = result['article_id']
                structured_data = result['structured_data']
                quality = result['quality_score']
                main_points = structured_data.get('main_points', [])

                if not main_points or len(main_points) < 2 or quality < 0.5:
                    db.mark_processed(article_id, 'stage1', 'skipped')
                    skipped += 1
                    continue

                total_quality_score += quality

                df_row = {
                    'article_id': article_id,
                    'title': result['title'],
                    'text': result['text'],
                    'label': result['label']
                }

                for i, point_data in enumerate(main_points[:5], 1):
                    df_row[f'claim_{i}'] = point_data.get('point', '')
                for i in range(len(main_points) + 1, 6):
                    df_row[f'claim_{i}'] = None

                db.log_claims_and_triplets(article_id, structured_data)
                db.mark_processed(article_id, 'stage1', 'success')
                current_batch_data.append(df_row)
                successful += 1
            if current_batch_data:
                print(f"\nðŸ’¾ Saving batch {batch_start // batch_size + 1}/{len(articles_to_process) // batch_size + 1}...")
                new_df = pd.DataFrame(current_batch_data)
                existing_df = pd.concat([existing_df, new_df]).drop_duplicates(subset=['article_id'], keep='last')

                try:
                    existing_df.reindex(columns=header).to_parquet(output_file, index=False, engine='pyarrow')
                    print(f"  Saved. Total articles in file: {len(existing_df)}")
                except Exception as e:
                    print(f"    Failed to save Parquet file: {e}")
                    print("     Saving as fallback CSV and continuing...")
                    existing_df.reindex(columns=header).to_csv("step1_claims_data_FALLBACK.csv", index=False)

    processor.unload()
    total_processed = successful + skipped
    final_success_rate = (successful / total_processed * 100) if total_processed > 0 else 0
    avg_quality = (total_quality_score / successful) if successful > 0 else 0

    print(f"\n{'='*70}")
    print(f" Stage 1 Complete ")
    print(f"   Successfully processed: {successful}")
    print(f"   Skipped: {skipped}")
    print(f"   Sucess rate on this run): {final_success_rate:.1f}%")
    print(f"   Average quality score: {avg_quality:.2f}/1.00")
    print(f"{'='*70}")

try:
    pd.read_csv("news_df.csv")
    print("Found news_df.csv")
except FileNotFoundError:
    print("file not found.")

run_stage_1_ultra_reliable("news_df.csv", batch_size=30) 
